{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7e8d2-709a-43b7-a077-0fc41c86e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hybrid Geostatistical–Deep Learning Framework for Geochemical Characterization in Historical Mine Tailings\n",
    "\n",
    "Author: Keyumars Anvari\n",
    "Supervisor: Professor Jörg Benndorf\n",
    "Affiliation: Department of Mine Surveying and Geodesy,\n",
    "             TU Bergakademie Freiberg, 09599 Freiberg, Germany\n",
    "\n",
    "Purpose:\n",
    "--------\n",
    "This code demonstrates the GCNN–RNN hybrid workflow described in our paper (see Algorithm 1 and Methods).\n",
    "The example uses synthetic data—replace these with your own geospatial/geochemical samples for real applications.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import logging\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import cdist\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv1D, LSTM, Dense, Dropout, Bidirectional, LeakyReLU\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "def set_random_seeds(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "set_random_seeds()\n",
    "\n",
    "# ======== Algorithm 1 Step 1: Data Preparation ========\n",
    "# (Replace with your own data for research use)\n",
    "df_known = pd.DataFrame({\n",
    "    'X': np.random.rand(50),\n",
    "    'Y': np.random.rand(50),\n",
    "    'Concentration': np.random.rand(50)\n",
    "})\n",
    "df_unknown = pd.DataFrame({\n",
    "    'X': np.random.rand(20),\n",
    "    'Y': np.random.rand(20)\n",
    "})\n",
    "\n",
    "# ======== Algorithm 1 Step 2: Compute Geostatistical Feature ========\n",
    "def spherical_variogram(h, nugget, sill, range_):\n",
    "    h = np.asarray(h)\n",
    "    gamma = np.where(h <= range_,\n",
    "                     nugget + (sill - nugget) * (1.5 * (h / range_) - 0.5 * (h / range_) ** 3),\n",
    "                     sill)\n",
    "    return gamma\n",
    "\n",
    "def compute_covariance_matrix(coordinates, variogram_params):\n",
    "    distances = cdist(coordinates, coordinates)\n",
    "    gamma = spherical_variogram(distances, variogram_params['nugget'], variogram_params['sill'], variogram_params['range'])\n",
    "    return variogram_params['sill'] - gamma\n",
    "\n",
    "variogram_params = {'nugget': 0.1, 'sill': 1.0, 'range': 0.5}\n",
    "coords_all = pd.concat([df_known[['X', 'Y']], df_unknown[['X', 'Y']]], ignore_index=True)\n",
    "cov_matrix = compute_covariance_matrix(coords_all.values, variogram_params)\n",
    "avg_cov = cov_matrix.mean(axis=1)\n",
    "coords_all['Average_Covariance'] = avg_cov\n",
    "df_known['Average_Covariance'] = coords_all['Average_Covariance'].iloc[:len(df_known)].values\n",
    "df_unknown['Average_Covariance'] = coords_all['Average_Covariance'].iloc[len(df_known):].values\n",
    "\n",
    "# ======== Algorithm 1 Step 3: Build Model Inputs ========\n",
    "X = df_known[['Concentration', 'X', 'Y', 'Average_Covariance']].values\n",
    "y = df_known[['Concentration']].values\n",
    "X_unknown = df_unknown[['X', 'Y', 'Average_Covariance']].values\n",
    "scaler_X = StandardScaler().fit(X)\n",
    "scaler_y = StandardScaler().fit(y)\n",
    "X_scaled = scaler_X.transform(X)\n",
    "y_scaled = scaler_y.transform(y)\n",
    "X_unknown_scaled = scaler_X.transform(np.hstack([np.zeros((X_unknown.shape[0], 1)), X_unknown]))\n",
    "\n",
    "# ======== Algorithm 1 Step 4: Split for Training/Validation ========\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "X_train = X_train.reshape(-1, 1, X_train.shape[1])\n",
    "X_val = X_val.reshape(-1, 1, X_val.shape[1])\n",
    "\n",
    "# ======== Algorithm 1 Step 5: Define and Train GCNN–RNN Model ========\n",
    "class NaNDetector(Callback):\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        loss = logs.get('loss') if logs else None\n",
    "        if loss is not None and np.isnan(loss):\n",
    "            logging.error(f\"NaN loss detected at batch {batch}. Stopping training.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(1, X_train.shape[2])),\n",
    "    Conv1D(filters=32, kernel_size=1, activation='linear', kernel_regularizer=l2(1e-4)),\n",
    "    LeakyReLU(),\n",
    "    Dropout(0.2),\n",
    "    Bidirectional(LSTM(32, return_sequences=False)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(optimizer=Adam(1e-4), loss='mse', metrics=['mae'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', patience=10, factor=0.5, min_lr=1e-6),\n",
    "    NaNDetector()\n",
    "]\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=200,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ======== Algorithm 1 Step 6: Predict on Unknowns and Visualize ========\n",
    "X_unknown_for_pred = X_unknown_scaled.reshape(-1, 1, X_unknown_scaled.shape[1])\n",
    "pred_unknown_scaled = model.predict(X_unknown_for_pred)\n",
    "pred_unknown = scaler_y.inverse_transform(pred_unknown_scaled)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(pred_unknown, bins=15, alpha=0.7)\n",
    "plt.title(\"Predicted Concentrations (Unknown Points)\")\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Optionally: Save predictions\n",
    "# df_unknown['Predicted_Concentration'] = pred_unknown.flatten()\n",
    "# df_unknown.to_csv('predictions_demo.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
